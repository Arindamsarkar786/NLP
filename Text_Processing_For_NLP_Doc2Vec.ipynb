{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzpAS6Bx08WG"
      },
      "source": [
        "# Importing neccessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qgL4fu5W08WM",
        "outputId": "63e99b9e-da5f-4e72-bee2-9bd0a7f655b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFKGMBKU08WO"
      },
      "source": [
        "# Sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zhZXqit608WP"
      },
      "outputs": [],
      "source": [
        "text = \"Doc2Vec is used for creating document embeddings. It captures the context of entire documents.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3OKhoHL08WQ"
      },
      "source": [
        "# Tokenize into sentences and words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TIKUw5mw08WQ",
        "outputId": "9c453b7d-2b51-44a1-d365-15813c7fed97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize into sentences and words\n",
            "Tokenized sentences: [['doc2vec', 'is', 'used', 'for', 'creating', 'document', 'embeddings', '.'], ['it', 'captures', 'the', 'context', 'of', 'entire', 'documents', '.']]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokenize into sentences and words\")\n",
        "sentences = sent_tokenize(text)\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "print(\"Tokenized sentences:\", tokenized_sentences)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgamW7Cu08WS"
      },
      "source": [
        "# Prepare tagged documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f_Vu1Pnr08WS",
        "outputId": "00d0a59c-b967-4956-9895-f8a016e4f741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare tagged documents\n"
          ]
        }
      ],
      "source": [
        "print(\"Prepare tagged documents\")\n",
        "tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(tokenized_sentences)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llfGR7M008WT"
      },
      "source": [
        "# Train the Doc2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IQpQrgcv08WU",
        "outputId": "e7f7bd3c-be33-4269-a363-51af2963bd84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train the Doc2Vec model\n",
            "Doc2Vec model trained successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Train the Doc2Vec model\")\n",
        "model = Doc2Vec(vector_size=100, window=5, min_count=1, dm=1, epochs=20)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "print(\"Doc2Vec model trained successfully\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kktIKiBE08WU"
      },
      "source": [
        "# Infer document vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "23DUsRgO08WU",
        "outputId": "037e6117-e71e-40f5-f86a-816b6d8b1c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer document vectors\n",
            "Inferred document vector: [-2.1985010e-04  7.4254553e-04 -3.7655002e-04 -4.7427844e-03\n",
            " -6.1516929e-04 -4.7957948e-03  4.5565730e-03 -4.4531059e-03\n",
            "  1.9111754e-03 -2.2909988e-03  4.2157425e-03  2.0011002e-04\n",
            " -3.0572752e-03  3.2754710e-03 -1.7289891e-03  2.2832949e-03\n",
            " -4.7926703e-03 -3.3135521e-03  1.9340445e-03  2.2456876e-03\n",
            "  1.4699302e-03  2.3665234e-04  7.2702806e-04  3.0257553e-03\n",
            "  3.5258878e-03 -4.2519108e-03  2.2594025e-03 -4.7590686e-03\n",
            " -5.2906189e-04 -4.6576548e-04 -2.8706507e-03  9.7874994e-04\n",
            " -4.6215942e-03  4.0019699e-03  2.6565293e-04  4.2037046e-04\n",
            "  3.1077662e-03 -2.5504152e-03  1.7726002e-03 -3.7269073e-03\n",
            "  2.6118618e-03 -3.9619561e-03  3.5981606e-03 -2.7910841e-03\n",
            " -4.6147662e-03  4.3655825e-03 -1.7558348e-04 -4.7953711e-03\n",
            "  3.1840873e-03 -7.0523476e-04  1.8381287e-04 -4.8049055e-03\n",
            " -3.7377537e-04  1.4850649e-03  3.4665328e-03 -3.0171768e-05\n",
            "  3.4661076e-03 -3.6522178e-03 -3.7894486e-03  2.3849623e-03\n",
            "  8.8178762e-04 -1.4749068e-03 -4.4402778e-03  3.5464072e-03\n",
            "  3.4242482e-03 -1.2647389e-03 -9.1010908e-04  2.2038631e-03\n",
            "  1.3843278e-03  4.8068995e-03  9.2865742e-04 -3.9950125e-03\n",
            "  2.7038751e-03  4.6891118e-03 -4.9378155e-03 -2.9582088e-03\n",
            "  8.2725810e-04 -3.4905430e-03 -1.5052983e-03  4.0377406e-03\n",
            " -4.6803430e-03  9.5371832e-04  3.9029787e-03 -2.5274535e-03\n",
            "  2.1386323e-03  2.0660949e-03 -4.0479358e-03 -1.1850989e-03\n",
            "  3.4756383e-03  9.2569593e-04  1.8022147e-03 -3.3209210e-03\n",
            "  9.1630191e-04  1.5581907e-03  7.3504128e-04  1.0290444e-03\n",
            " -3.1215388e-03  3.4819954e-04 -2.9216828e-03 -3.5877561e-03]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Infer document vectors\")\n",
        "doc_vector = model.infer_vector(word_tokenize(\"Doc2Vec is a powerful tool for document embeddings.\"))\n",
        "print(\"Inferred document vector:\", doc_vector)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABqsILue08WV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}