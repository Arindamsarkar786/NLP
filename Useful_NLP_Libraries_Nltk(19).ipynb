{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20PRzb8NEkcI"
      },
      "source": [
        "# Importing neccessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ew_4nVlnEkcN",
        "outputId": "efea1d12-87ed-4b3f-db30-5a2d140e36ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK using pip\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v4E8UDEDEkcS",
        "outputId": "f68efb15-65b6-4397-9602-a2afe1df4cd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import NLTK\n",
        "import nltk\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj9K-i-dEkcT"
      },
      "source": [
        "# Sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mYn_y4EVEkcU"
      },
      "outputs": [],
      "source": [
        "text = \"NLTK is a powerful tool for natural language processing. It can tokenize sentences and words. NLTK includes various NLP libraries for text analysis.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfFD-tV1EkcU"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I-ZX0L7DEkcU",
        "outputId": "e9241d22-d5c7-430c-fc44-4372381257bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'natural', 'language', 'processing', '.', 'It', 'can', 'tokenize', 'sentences', 'and', 'words', '.', 'NLTK', 'includes', 'various', 'NLP', 'libraries', 'for', 'text', 'analysis', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J978GXAaEkcV"
      },
      "source": [
        "# Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zeZvHbyZEkcV",
        "outputId": "619c6fed-2a86-4a4d-95be-b0f44c90afd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('It', 'PRP'), ('can', 'MD'), ('tokenize', 'VB'), ('sentences', 'NNS'), ('and', 'CC'), ('words', 'NNS'), ('.', '.'), ('NLTK', 'NNP'), ('includes', 'VBZ'), ('various', 'JJ'), ('NLP', 'NNP'), ('libraries', 'NNS'), ('for', 'IN'), ('text', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tags:\", pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noyjJ7GhEkcW"
      },
      "source": [
        "# Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ajdC0arvEkcW",
        "outputId": "3eebfab2-7e26-48ee-9785-a885efa76d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['nltk', 'is', 'a', 'power', 'tool', 'for', 'natur', 'languag', 'process', '.', 'it', 'can', 'token', 'sentenc', 'and', 'word', '.', 'nltk', 'includ', 'variou', 'nlp', 'librari', 'for', 'text', 'analysi', '.']\n",
            "Lemmatized Words: ['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'natural', 'language', 'processing', '.', 'It', 'can', 'tokenize', 'sentence', 'and', 'word', '.', 'NLTK', 'includes', 'various', 'NLP', 'library', 'for', 'text', 'analysis', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpEtOmx7EkcX"
      },
      "source": [
        "# Stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jlzYVSokEkcX",
        "outputId": "a3740e9c-8ac7-483b-d3ca-b1688995bc0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['NLTK', 'powerful', 'tool', 'natural', 'language', 'processing', '.', 'tokenize', 'sentences', 'words', '.', 'NLTK', 'includes', 'various', 'NLP', 'libraries', 'text', 'analysis', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TV9H6IsEkcX"
      },
      "source": [
        "# Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ViyczRxdEkcY",
        "outputId": "d950882e-c950-42d8-ff2a-c396a26cece4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution: <FreqDist with 22 samples and 26 outcomes>\n"
          ]
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist = FreqDist(words)\n",
        "print(\"Frequency Distribution:\", freq_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD3h7qHNEkcY"
      },
      "source": [
        "# Concordance and Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8vL_EGSlEkcY",
        "outputId": "d16808d2-21cc-4948-e229-01fb360f5edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 2 of 2 matches:\n",
            " NLTK is a powerful tool for natural langu\n",
            "t can tokenize sentences and words . NLTK includes various NLP libraries for t\n",
            "\n",
            "Concordance Result: None\n",
            "Similar Words: None\n"
          ]
        }
      ],
      "source": [
        "from nltk.text import Text\n",
        "\n",
        "text_object = Text(words)\n",
        "concordance_result = text_object.concordance(\"NLTK\")\n",
        "similar_words = text_object.similar(\"tool\")\n",
        "print(\"Concordance Result:\", concordance_result)\n",
        "print(\"Similar Words:\", similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1PIup8TEkcZ"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ol0IaukwEkcZ",
        "outputId": "d2209880-7e36-48eb-c4ff-68fcc0c17286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Score: {'neg': 0.0, 'neu': 0.745, 'pos': 0.255, 'compound': 0.6705}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_score = sia.polarity_scores(text)\n",
        "print(\"Sentiment Analysis Score:\", sentiment_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIENGowYEkcZ"
      },
      "source": [
        "# Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "X2HTvp1aEkca",
        "outputId": "6a036953-fe88-48ee-e5a7-6e1c5b6fcea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Result: (S\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  powerful/JJ\n",
            "  tool/NN\n",
            "  for/IN\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  ./.\n",
            "  It/PRP\n",
            "  can/MD\n",
            "  tokenize/VB\n",
            "  sentences/NNS\n",
            "  and/CC\n",
            "  words/NNS\n",
            "  ./.\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  includes/VBZ\n",
            "  various/JJ\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  libraries/NNS\n",
            "  for/IN\n",
            "  text/JJ\n",
            "  analysis/NN\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags_for_ner = pos_tag(tokens)\n",
        "ner_result = ne_chunk(pos_tags_for_ner)\n",
        "print(\"NER Result:\", ner_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2hYtKteEkca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}